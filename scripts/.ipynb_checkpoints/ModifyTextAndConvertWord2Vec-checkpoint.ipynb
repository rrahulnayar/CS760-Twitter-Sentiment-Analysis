{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import pp\n",
    "#from sklearn import preprocessing\n",
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import TweetTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataF = \"G:/edu/sem2/760/proj/CS760-Twitter-Sentiment-Analysis/data/\"\n",
    "all1F = dataF+'train_utf8.csv'\n",
    "all2F = dataF+'train_utf8.csv'\n",
    "samF = dataF+'sample.csv'\n",
    "modifiedF = dataF+'modified.csv'\n",
    "vecsF = dataF+'allVecs.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dt= pd.read_csv(samF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wasteWords =re.compile(\n",
    "    r'\\b(the|a|an|as|be|am|is|are|was|were|has|have|had|'+\n",
    "    r'at|from|to|in|under|before|after|near|far|away|for|and|'+\n",
    "    r'that|them|there|their|they|his|her|hers|him|it|you|your|yours)\\b'\n",
    ", re.I)\n",
    "\n",
    "#allow - & '\n",
    "badpunc = re.compile(\n",
    "    r'\\.|\\_|\\:|\\`|\\\"|\\~|\\!|\\@|\\#|\\$|\\%|\\^|\\*|\\(|\\)|\\+|\\=|\\{|\\}|\\[|\\]|'+\n",
    "    r'\\;|\\<|\\>|\\,|\\?|\\/|\\\\|\\|',re.I)\n",
    "apos = re.compile(r'\\'\\w*',re.I)\n",
    "wrdBrk1 = re.compile(r'(\\w)(\\W)',re.I)\n",
    "wrdBrk2 = re.compile(r'(\\W)(\\w)',re.I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#string corrections\n",
    "def modify(txt):\n",
    "    txt = txt.strip()\n",
    "    txt = txt.replace('&quot;',\"'\")\n",
    "    txt = txt.replace('&amp;','&')\n",
    "    txt = txt.replace('&lt;','<')\n",
    "    txt = txt.replace('&gt;','>')\n",
    "    #txt = re.sub(wasteWords,' ',txt)\n",
    "    txt = re.sub(wrdBrk1,r'\\1 \\2',txt)\n",
    "    txt = re.sub(wrdBrk2,r'\\1 \\2',txt)\n",
    "    txt = re.sub(badpunc,' ',txt)\n",
    "    txt = re.sub(r'\\s+',' ',txt)\n",
    "    \n",
    "    return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = dt['Sentiment'].map(lambda x: 'Y' if x==1 else 'N')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modified = dt['SentimentText'].map(modify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([labels,modified],axis=1).rename(columns={'Sentiment':'Label','SentimentText':'text'})\\\n",
    ".to_csv(modifiedF,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text): \n",
    "    tknzr = TweetTokenizer()\n",
    "    return tknzr.tokenize(text)\n",
    "\n",
    "#def stem(doc):\n",
    "#    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "en_stopwords = set(stopwords.words(\"english\")) \n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words = en_stopwords,\n",
    "    max_features=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'scipy.sparse.csr.csr_matrix'>\n",
      "(1000, 512)\n"
     ]
    }
   ],
   "source": [
    "vecs = vectorizer.fit_transform(dt['SentimentText'])\n",
    "print(type(vecs))\n",
    "print(vecs.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
